<html>

<head>
<title>Practical Machine Learning Assignment</title>
</head>

<body>

<p>This document sums up what I have done on the programming assignment for the course "Practical Machine Learning" on Coursera</p>

<h1>Step 0:</h1>

<p>Let's first import the library we will use. </p>
<!--begin.rcode
library(lattice); library(ggplot2); library(caret);library(kernlab); library(randomForest)
end.rcode-->

<p>And let's fix the seed to make the experience reproductive. </p>
<!--begin.rcode
set.seed(32323)
end.rcode-->

<h1>Step 1: Loading the data</h1>

<p>Let's load the train and test datasets.</p>

<!--begin.rcode
# Load the training dataset:
dataset  <- read.csv("pml-training.csv", na.strings=c("NA",""), strip.white=TRUE)
dim(dataset)

# Load the test dataset
dataset_test <- read.csv("pml-testing.csv", na.strings=c("NA",""), strip.white=T)
dim(dataset_test)
end.rcode-->

<p>We have <b>19622 training examples</b> composed of the recording of <b>160 measures</b>. <br/>
We have <b>20 test examples</b>.</p>

<h1>Step 2: Cleaning the data</h1>

<p>By looking at the data we can notice that there are a lot of empty columns.<br/>
Let's remove them. </p>

<!--begin.rcode
# Cleaning the data:
isNA <- apply(dataset, 2, function(x) { sum(is.na(x)) })

# For the training dataset:
dataset <- subset(dataset[, which(isNA == 0)], 
                    select=-c(X, user_name, new_window, num_window, 
                              raw_timestamp_part_1, raw_timestamp_part_2, 
                              cvtd_timestamp))
dim(dataset)

# For the test dataset:
dataset_test <- subset(dataset_test[, which(isNA == 0)], 
                        select=-c(X, user_name, new_window, num_window,
                                  raw_timestamp_part_1, raw_timestamp_part_2,
                                  cvtd_timestamp))
end.rcode-->

<p>We have reduced the training set to <b>19622 training examples</b> composed of the recording of <b>53 usable measures</b>.</p>


<h1>Step 3: Creation of the training set and validation set</h1>

<p>In order to measure the performances of our future predictor we split the dataset into a training set and a validation set. The first one will be used to train the predictor, while the second one will be used to assess the performances.<br/>
The split used is: <b>80% </b> for the training set and <b>20%</b> for the validation set.</p>

<!--begin.rcode
# Spliting the dataset:
inTrain <- createDataPartition(dataset$classe, p=0.8, list=FALSE)
train_set <- dataset[inTrain,]
valid_set <- dataset[-inTrain,]
end.rcode-->

<h1>Step 4: Training some predictors</h1>

<h2> Test 1: Random Forest:</h1>
<p>We first try an powerful yet easy to parametrize predictor to have an idea of what performance could be obtained. <br/>
We train a <b>Random Forest Classifier</b>.</p>

<!--begin.rcode
# Test 1: Random forest classifier:
ctrl <- trainControl(allowParallel=TRUE, method="cv", number=4)
model <- train(classe ~ ., data=train_set, model="rf", trControl=ctrl)
predictor <- predict(model, newdata=valid_set)
end.rcode-->

<p>Let's now assess the generalization performance of this classifier by computing the error made on the validation set.</p>

<!--begin.rcode
# Error on valid_set:
sum(predictor == valid_set$classe) / length(predictor)
confusionMatrix(valid_set$classe, predictor)$table
end.rcode-->

<p>We have a validation error of <b>99.29%</b>.<br/>
We can consider this classifier as good.</p>

<p>Let's see what are the prediction for the test set.</p>

<!--begin.rcode
# Prediction on the test set:
predict(model, newdata=dataset_test)
end.rcode-->

<p>We have the following prediction: <br/>
<b>  B A B A A E D B A A B C B A E E A B B B </b></p>

<h2> Test 2: SVM on lower dimension training examples</h1>

<p>The random forest appears to be a good predictor but it is quite slow to train on the 53 dimensions vectors of our dataset. <br/>
Let's assess the performance of an SVM trained only on the ten most important parameters. <br/>
We train a <b>SVM Classifier</b>.</p>

<!--begin.rcode
# Most important variable for the ramdom forest predictor:
varImp(model)

# Let us reduce the training dataset to those ten variables
dataset_small <- subset(dataset, 
                         select=c(roll_belt, pitch_forearm, yaw_belt,
                                  magnet_dumbbell_y, pitch_belt, 
                                  magnet_dumbbell_z, roll_forearm,
                                  accel_dumbbell_y, roll_dumbbell,
                                  magnet_dumbbell_x,classe))
dim(dataset_small)
end.rcode-->

<p>We now have <b>19622 training examples</b> composed of the recording of <b>10 measures</b>. (11th parammeter is the class)</p>

<p>Let's train the SVM on this "small" dataset.</p>

<!--begin.rcode
# Training an SVM on the small dataset
svm <- train(classe ~ ., data=dataset_small[inTrain,], model="svm", trControl=ctrl)
predictor_svm <- predict(svm, newdata=valid_set)
end.rcode-->

<p>Let's now assess the generalization performance of this classifier by computing the error made on the validation set.</p>

<!--begin.rcode
# Error on valid_set:
sum(predictor_svm == valid_set$classe) / length(predictor_svm)
confusionMatrix(valid_set$classe, predictor_svm)$table
end.rcode-->

<p>We have a validation error of <b>98.45%</b>.<br/>
The performance are slightly lower but the training is way faster. <br/>
Depending on the application the trade-off could be interesting</p>

<p>Let's see what are the prediction for the test set.</p>

<!--begin.rcode
# Prediction on the test set:
predict(model, newdata=dataset_test)
end.rcode-->

<p>We have the following prediction: <br/>
<b> B A B A A E D B A A B C B A E E A B B B </b> <br/>
We have the same prediction with the random forest trained on the full dataset and with the SVM trained with the reduced dataset. Again the trade-off "speed of training versus "classification performances" could be interesting depending on the application.</p>

</body>
</html>
