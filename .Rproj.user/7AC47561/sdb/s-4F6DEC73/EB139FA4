{
    "contents" : "<html>\n\n<head>\n<title>Practical Machine Learning Assignment</title>\n</head>\n\n<body>\n\n<p>This document sums up what I have done on the programming assignment for the course \"Practical Machine Learning\" on Coursera</p>\n\n<h1>Step 0:</h1>\n\n<p>Let's first import the library we will use. </p>\n<!--begin.rcode\nlibrary(lattice); library(ggplot2); library(caret);library(kernlab); library(randomForest)\nend.rcode-->\n\n<p>And let's fix the seed to make the experience reproductive. </p>\n<!--begin.rcode\nset.seed(32323)\nend.rcode-->\n\n<h1>Step 1: Loading the data</h1>\n\n<p>Let's load the train and test datasets.</p>\n\n<!--begin.rcode\n# Load the training dataset:\ndataset  <- read.csv(\"pml-training.csv\", na.strings=c(\"NA\",\"\"), strip.white=TRUE)\ndim(dataset)\n\n# Load the test dataset\ndataset_test <- read.csv(\"pml-testing.csv\", na.strings=c(\"NA\",\"\"), strip.white=T)\ndim(dataset_test)\nend.rcode-->\n\n<p>We have <b>19622 training examples</b> composed of the recording of <b>160 measures</b>. <br/>\nWe have <b>20 test examples</b>.</p>\n\n<h1>Step 2: Cleaning the data</h1>\n\n<p>By looking at the data we can notice that there are a lot of empty columns.<br/>\nLet's remove them. </p>\n\n<!--begin.rcode\n# Cleaning the data:\nisNA <- apply(dataset, 2, function(x) { sum(is.na(x)) })\n\n# For the training dataset:\ndataset <- subset(dataset[, which(isNA == 0)], \n                    select=-c(X, user_name, new_window, num_window, \n                              raw_timestamp_part_1, raw_timestamp_part_2, \n                              cvtd_timestamp))\ndim(dataset)\n\n# For the test dataset:\ndataset_test <- subset(dataset_test[, which(isNA == 0)], \n                        select=-c(X, user_name, new_window, num_window,\n                                  raw_timestamp_part_1, raw_timestamp_part_2,\n                                  cvtd_timestamp))\nend.rcode-->\n\n<p>We have reduced the training set to <b>19622 training examples</b> composed of the recording of <b>53 usable measures</b>.</p>\n\n\n<h1>Step 3: Creation of the training set and validation set</h1>\n\n<p>In order to measure the performances of our future predictor we split the dataset into a training set and a validation set. The first one will be used to train the predictor, while the second one will be used to assess the performances.<br/>\nThe split used is: <b>80% </b> for the training set and <b>20%</b> for the validation set.</p>\n\n<!--begin.rcode\n# Spliting the dataset:\ninTrain <- createDataPartition(dataset$classe, p=0.8, list=FALSE)\ntrain_set <- dataset[inTrain,]\nvalid_set <- dataset[-inTrain,]\nend.rcode-->\n\n<h1>Step 4: Training some predictors</h1>\n\n<h2> Test 1: Random Forest:</h1>\n<p>We first try an powerful yet easy to parametrize predictor to have an idea of what performance could be obtained. <br/>\nWe train a <b>Random Forest Classifier</b>.</p>\n\n<!--begin.rcode\n# Test 1: Random forest classifier:\nctrl <- trainControl(allowParallel=TRUE, method=\"cv\", number=4)\nmodel <- train(classe ~ ., data=train_set, model=\"rf\", trControl=ctrl)\npredictor <- predict(model, newdata=valid_set)\nend.rcode-->\n\n<p>Let's now assess the generalization performance of this classifier by computing the error made on the validation set.</p>\n\n<!--begin.rcode\n# Error on valid_set:\nsum(predictor == valid_set$classe) / length(predictor)\nconfusionMatrix(valid_set$classe, predictor)$table\nend.rcode-->\n\n<p>We have a validation error of <b>99.29%</b>.<br/>\nWe can consider this classifier as good.</p>\n\n<p>Let's see what are the prediction for the test set.</p>\n\n<!--begin.rcode\n# Prediction on the test set:\npredict(model, newdata=dataset_test)\nend.rcode-->\n\n<p>We have the following prediction: <br/>\n<b>  B A B A A E D B A A B C B A E E A B B B </b></p>\n\n<h2> Test 2: SVM on lower dimension training examples</h1>\n\n<p>The random forest appears to be a good predictor but it is quite slow to train on the 53 dimensions vectors of our dataset. <br/>\nLet's assess the performance of an SVM trained only on the ten most important parameters. <br/>\nWe train a <b>SVM Classifier</b>.</p>\n\n<!--begin.rcode\n# Most important variable for the ramdom forest predictor:\nvarImp(model)\n\n# Let us reduce the training dataset to those ten variables\ndataset_small <- subset(dataset, \n                         select=c(roll_belt, pitch_forearm, yaw_belt,\n                                  magnet_dumbbell_y, pitch_belt, \n                                  magnet_dumbbell_z, roll_forearm,\n                                  accel_dumbbell_y, roll_dumbbell,\n                                  magnet_dumbbell_x,classe))\ndim(dataset_small)\nend.rcode-->\n\n<p>We now have <b>19622 training examples</b> composed of the recording of <b>10 measures</b>. (11th parammeter is the class)</p>\n\n<p>Let's train the SVM on this \"small\" dataset.</p>\n\n<!--begin.rcode\n# Training an SVM on the small dataset\nsvm <- train(classe ~ ., data=dataset_small[inTrain,], model=\"svm\", trControl=ctrl)\npredictor_svm <- predict(svm, newdata=valid_set)\nend.rcode-->\n\n<p>Let's now assess the generalization performance of this classifier by computing the error made on the validation set.</p>\n\n<!--begin.rcode\n# Error on valid_set:\nsum(predictor_svm == valid_set$classe) / length(predictor_svm)\nconfusionMatrix(valid_set$classe, predictor_svm)$table\nend.rcode-->\n\n<p>We have a validation error of <b>98.45%</b>.<br/>\nThe performance are slightly lower but the training is way faster. <br/>\nDepending on the application the trade-off could be interesting</p>\n\n<p>Let's see what are the prediction for the test set.</p>\n\n<!--begin.rcode\n# Prediction on the test set:\npredict(model, newdata=dataset_test)\nend.rcode-->\n\n<p>We have the following prediction: <br/>\n<b> B A B A A E D B A A B C B A E E A B B B </b> <br/>\nWe have the same prediction with the random forest trained on the full dataset and with the SVM trained with the reduced dataset. Again the trade-off \"speed of training versus \"classification performances\" could be interesting depending on the application.</p>\n\n</body>\n</html>\n",
    "created" : 1403416196292.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1840865218",
    "id" : "EB139FA4",
    "lastKnownWriteTime" : 1403416061,
    "path" : "E:/Coursera/Practical Machine Learning/Test/WriteUp.Rhtml",
    "project_path" : "WriteUp.Rhtml",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_html"
}